---
description: é­”æ”¹openai apiåè®®è¯´æ˜
date: '2025-11-04'
author: 'pxx'
categories:
  - LLM
  - RAG
published: false
---



# API ä½¿ç”¨è¯´æ˜æ–‡æ¡£

## æ¦‚è¿°

æœ¬ API å®Œå…¨å…¼å®¹ OpenAI Chat Completions åè®®ï¼Œå¯ç›´æ¥ä½¿ç”¨ OpenAI SDK è¿›è¡Œè°ƒç”¨ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ‰©å±•äº†ä»¥ä¸‹åŠŸèƒ½ï¼š

- **æ™ºèƒ½çŸ¥è¯†åº“æ£€ç´¢**ï¼šæ”¯æŒ RAG/KAG å¤šæ¨¡æ€æ–‡æ¡£é—®ç­”
- **å›¾åƒç†è§£èƒ½åŠ›**ï¼šè‡ªåŠ¨ OCR è¯†åˆ«å¹¶ç†è§£å›¾ç‰‡å†…å®¹
- **å·¥å…·è°ƒç”¨èƒ½åŠ›**ï¼šé›†æˆ MCP åè®®ï¼Œæ”¯æŒå¤šè½®è‡ªä¸»å·¥å…·è°ƒç”¨

**å…¼å®¹æ€§è¯´æ˜**ï¼šæ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨ç°æœ‰çš„ OpenAI SDK ä»£ç ï¼Œåªéœ€ä¿®æ”¹ `base_url` å’Œ `api_key` å³å¯æ— ç¼è¿ç§»ã€‚



**Base URL**ï¼š`http://<your-host>/v1`

**ä¸»è¦æ¥å£**ï¼š

- `POST /v1/chat/completions` â€”â€” å…¼å®¹ OpenAI ChatCompletionsï¼Œæ‰©å±•å¤šè½®æ€è€ƒã€RAG/KAGã€MCPã€å›¾åƒè¯†åˆ«
- `POST /v1/mcp/connect` â€”â€” è¿æ¥ MCP å·¥å…·æœåŠ¡å™¨
- `GET  /v1/mcp/tools` â€”â€” æŸ¥çœ‹å·²åŠ è½½çš„ MCP å·¥å…·åˆ—è¡¨

**é‰´æƒæ–¹å¼**ï¼š`Authorization: Bearer <API_KEY>`

## å¿«é€Ÿå¼€å§‹

**åŸºç¡€å¯¹è¯**

æ›´å¤šè¯¦æƒ…è¯·æŸ¥çœ‹ [Chat Completions | OpenAI API Reference](https://platform.openai.com/docs/api-reference/chat/create)

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://your-domain:port/v1",
    api_key="YOUR_API_KEY"
)

# æ ‡å‡†è°ƒç”¨ï¼ˆå®Œå…¨å…¼å®¹ OpenAIï¼‰
response = client.chat.completions.create(
    model="your-model-name",
    messages=[{"role": "user", "content": "Hello"}]
)
```

**è®¤è¯æ–¹å¼**

ä½¿ç”¨æ ‡å‡†çš„ Bearer Token è®¤è¯ï¼š

```http
Authorization: Bearer YOUR_API_KEY
```

## æ‰©å±•åŠŸèƒ½è¯¦è§£

### 1. çŸ¥è¯†åº“æ£€ç´¢ï¼ˆRAG/KAGï¼‰

#### åŠŸèƒ½è¯´æ˜

é€šè¿‡ `extra.file_search` å­—æ®µå¯ç”¨çŸ¥è¯†åº“æ£€ç´¢ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ï¼š
- è¯†åˆ«é¡¹ç›®ç±»å‹ï¼ˆRAG æˆ– KAGï¼‰
- å°†ç›¸å…³æ–‡æ¡£æ³¨å…¥å¯¹è¯ä¸Šä¸‹æ–‡
- åŸºäºå¢å¼ºåçš„ä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”

#### å‚æ•°é…ç½®

```json
{
  "model": "your-model-name",
  "messages": [
    {"role": "user", "content": "è¯·åˆ†æå…¬å¸Q3å­£åº¦çš„é”€å”®æ•°æ®"}
  ],
  "extra": {
    "file_search": {
      "vector_store_ids": ["project-id-1", "project-id-2"],
      "max_num_results": 5
    }
  }
}
```

| å‚æ•° | ç±»å‹ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|------|--------|
| `vector_store_ids` | array | çŸ¥è¯†åº“é¡¹ç›® ID åˆ—è¡¨ï¼Œæ”¯æŒå¤šé¡¹ç›®è”åˆæ£€ç´¢ | **å¿…å¡«** |
| `max_num_results` | integer | è¿”å›çš„æœ€å¤§æ–‡æ¡£æ•° | RAG: 5<br>KAG: 20 |

#### RAG ä¸ KAG çš„åŒºåˆ«

**RAG æ¨¡å¼**ï¼ˆä¼ ç»Ÿå‘é‡æ£€ç´¢ï¼‰ï¼š
- æ£€ç´¢æ¥å£ï¼š`POST /query/query`
- è¿”å›ç»“æœï¼šç›¸ä¼¼åº¦æ’åºçš„æ–‡æ¡£ç‰‡æ®µ
- é€‚ç”¨åœºæ™¯ï¼šé€šç”¨æ–‡æ¡£é—®ç­”ã€çŸ¥è¯†æŸ¥è¯¢

**KAG æ¨¡å¼**ï¼ˆçŸ¥è¯†å¢å¼ºå›¾è°±æ£€ç´¢ï¼‰ï¼š

- æ£€ç´¢æ¥å£ï¼š`POST /kag/reasoning_context`
- è¿”å›ç»“æœï¼šåŒ…å«æ–‡æ¡£ç‰‡æ®µã€çŸ¥è¯†å›¾è°±å…³ç³»ã€æ¨ç†è¿‡ç¨‹
- é€‚ç”¨åœºæ™¯ï¼šå¤æ‚æ¨ç†ã€å¤šè·³é—®ç­”ã€å…³ç³»åˆ†æ

ç³»ç»Ÿä¼šæ ¹æ®é¡¹ç›®é…ç½®ï¼ˆ`grag_enable` å­—æ®µï¼‰è‡ªåŠ¨é€‰æ‹©æ£€ç´¢æ¨¡å¼ã€‚

#### æµå¼å“åº”ä¸­çš„æ£€ç´¢ç»“æœ

åœ¨ `stream: true` æ¨¡å¼ä¸‹ï¼Œæ£€ç´¢åˆ°çš„æ–‡æ¡£ä¼šä¼˜å…ˆä»¥ç‰¹æ®Šæ ¼å¼è¿”å›ï¼š

```json
{
  "id": "chatcmpl-abc123",
  "choices": [{
    "index": 0,
    "delta": {
      "role": "file_search",
      "annotations": [{
        "id": "chunk-123",
        "filename": "è´¢åŠ¡æŠ¥è¡¨.pdf (ID: file-456)",
        "content": "Q3å­£åº¦è¥æ”¶å¢é•¿15%..."
      }]
    },
    "finish_reason": null
  }]
}
```

#### KAG æ£€ç´¢ç»“æœç»“æ„

KAG æ¨¡å¼ä¼šè¿”å›ä¸‰ç±»ä¿¡æ¯ï¼š

1. **æ–‡æ¡£ç‰‡æ®µ**ï¼ˆchunksï¼‰
   
   ```json
   {
     "id": "chunk-123",
     "filename": "æŠ¥å‘Š.pdf (ID: file-456)",
     "content": "åŸå§‹æ–‡æ¡£å†…å®¹..."
   }
   ```
   
2. **çŸ¥è¯†å›¾è°±**ï¼ˆgraphï¼‰
   ```json
   {
     "id": "graph_0",
     "filename": "çŸ¥è¯†å›¾è°±",
     "content": "**ç›¸å…³å®ä½“ï¼š**\n- é”€å”®é¢\n- Q3å­£åº¦\n\n**çŸ¥è¯†å…³ç³»ï¼š**\n- é”€å”®é¢ â†’ å¢é•¿ â†’ 15%"
   }
   ```

3. **æ¨ç†è¿‡ç¨‹**ï¼ˆmemoryï¼‰
   ```json
   {
     "id": "memory_0",
     "filename": "æ¨ç†è¿‡ç¨‹ (GraphRAGExecutor)",
     "content": "åŸºäºçŸ¥è¯†å›¾è°±åˆ†æï¼ŒQ3å­£åº¦çš„æ ¸å¿ƒæ•°æ®åŒ…æ‹¬..."
   }
   ```

#### Python ç¤ºä¾‹

```python
from openai import OpenAI

client = OpenAI(base_url=f"<server_url>/v1", api_key="<api-key>")

# å¤šé¡¹ç›®è”åˆæ£€ç´¢
response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "user", "content": "å¯¹æ¯”æŠ€æœ¯æ–‡æ¡£å’Œè´¢åŠ¡æ•°æ®"}
    ],
    extra={
        "file_search": {
            "vector_store_ids": ["tech-docs", "finance-docs"],
            "max_num_results": 10
        }
    },
    stream=True  # æµå¼æŸ¥çœ‹æ£€ç´¢è¿‡ç¨‹
)

for chunk in response:
    delta = chunk.choices[0].delta
    if getattr(delta, "reasoning_content", None):
        print(delta.reasoning_content, end="", flush=True)
        reasoning_text += delta.reasoning_content
    if getattr(delta, "role", None) == "file_search":
        annotations = getattr(delta, "annotations", [])
        if annotations:
            print("\nğŸ“„ RAG æŸ¥è¯¢ç»“æœï¼š", flush=True)
            for i, ann in enumerate(annotations, start=1):
                doc_id = ann.get("id", "")
                filename = ann.get("filename", "")
                print(f"  {i}. æ–‡æ¡£ID: {doc_id}", flush=True)
                print(f"     æ–‡ä»¶å: {filename}", flush=True)
                # ä»…æ˜¾ç¤ºå†…å®¹å‰ 100 ä¸ªå­—ç¬¦ä½œä¸ºé¢„è§ˆ
                preview = ann.get("content", "")
                preview = preview.replace("\n", " ")[:100]
                print(f"     å†…å®¹é¢„è§ˆ: {preview}...", flush=True)
                print("  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€", flush=True)
    elif delta.content:
        print(delta.content, end="", flush=True)
```

#### éæµå¼å“åº”ä¸­çš„ Trace

åœ¨éæµå¼æ¨¡å¼ä¸‹ï¼Œå®Œæ•´çš„æ£€ç´¢è¿‡ç¨‹ä¼šè®°å½•åœ¨å“åº”çš„ `trace` å­—æ®µä¸­ï¼š

```json
{
  "id": "chatcmpl-abc123",
  "choices": [...],
  "trace": [
    {
      "type": "rag_context",
      "docs": [
        {
          "id": "chunk-123",
          "filename": "æŠ¥å‘Š.pdf",
          "content": "æ–‡æ¡£å†…å®¹..."
        }
      ]
    },
    {
      "type": "assistant",
      "step": 0,
      "message": {...}
    }
  ]
}
```

```python
from openai import OpenAI

client = OpenAI(base_url=f"<server_url>/v1", api_key="<api-key>")

# RAG æ£€ç´¢ç¤ºä¾‹
response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "user", "content": "æ€»ç»“å…¬å¸è´¢æŠ¥çš„æ ¸å¿ƒå†…å®¹"}
    ],
    extra={
        "file_search": {
            "vector_store_ids": ["finance-docs"],
            "max_num_results": 5
        }
    }
)

msg = response.choices[0].message
if hasattr(response, "trace"):
    for step in response.trace:
        if step["type"] == "rag_context":
            for doc in step["docs"]:
                print(f"ğŸ“„ {doc['filename']}: {doc['content'][:200]}...")
print(msg.reasoning_content if hasattr(msg, "trace") else "")
print(msg.content)
```



### 2. å›¾åƒè¯†åˆ«ï¼ˆOCRï¼‰

å‚è€ƒ [How to use vision-enabled chat models - Azure OpenAI in Microsoft Foundry Models | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/gpt-with-vision?view=foundry-classic&utm_source=chatgpt.com&tabs=rest)

#### åŠŸèƒ½è¯´æ˜

æ”¯æŒåœ¨æ¶ˆæ¯ä¸­ç›´æ¥åŒ…å«å›¾ç‰‡ï¼Œç³»ç»Ÿä¼šï¼š
1. è‡ªåŠ¨æå–æ‰€æœ‰ `image_url` ç±»å‹çš„å†…å®¹
2. è°ƒç”¨ OCR æœåŠ¡ï¼ˆ`/api/ocr`ï¼‰è¿›è¡Œæ–‡å­—è¯†åˆ«
3. å°†è¯†åˆ«ç»“æœä½œä¸ºç³»ç»Ÿæ¶ˆæ¯æ³¨å…¥å¯¹è¯
4. æ¨¡å‹åŸºäºå›¾ç‰‡æ–‡å­—å†…å®¹è¿›è¡Œç†è§£å’Œæ¨ç†

#### ä½¿ç”¨æ–¹æ³•

```python
response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "è¯·åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„æ–‡å­—å¹¶æ€»ç»“è¦ç‚¹"
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "data:image/png;base64,iVBORw0KGgo..."
                    }
                }
            ]
        }
    ]
)
```

#### å›¾ç‰‡æ ¼å¼è¦æ±‚

- **æ ¼å¼æ”¯æŒ**ï¼šPNGã€JPEGã€GIF ç­‰å¸¸è§æ ¼å¼
- **ç¼–ç æ–¹å¼**ï¼šä»…æ”¯æŒ Data URL æ ¼å¼ï¼ˆ`data:image/*;base64,...`ï¼‰
- **æ¨èå°ºå¯¸**ï¼š1024px ä»¥å†…ï¼ˆè‡ªåŠ¨å‹ç¼©ï¼‰

#### OCR é…ç½®å‚æ•°

OCR æœåŠ¡ä½¿ç”¨ä»¥ä¸‹é»˜è®¤é…ç½®ï¼š
- `mode`: `describe`ï¼ˆæè¿°æ¨¡å¼ï¼‰
- `base_size`: `1024`ï¼ˆåŸºç¡€å°ºå¯¸ï¼‰
- `image_size`: `640`ï¼ˆå›¾åƒå°ºå¯¸ï¼‰
- `crop_mode`: `true`ï¼ˆè£å‰ªæ¨¡å¼ï¼‰

#### å¤„ç†æµç¨‹

```
ç”¨æˆ·è¯·æ±‚ â†’ æå–å›¾ç‰‡ â†’ è°ƒç”¨ OCR API â†’ æ³¨å…¥è¯†åˆ«ç»“æœ â†’ æ¨¡å‹æ¨ç†
```

è¯†åˆ«ç»“æœä¼šä»¥å¦‚ä¸‹æ ¼å¼æ³¨å…¥ï¼š

```json
{
  "role": "system",
  "content": "[Image 1 OCR]: è¿™æ˜¯è¯†åˆ«åˆ°çš„æ–‡å­—å†…å®¹...\n[Image 2 OCR]: ç¬¬äºŒå¼ å›¾ç‰‡çš„å†…å®¹..."
}
```

#### å¤šå›¾ç‰‡å¤„ç†

```python
from openai import OpenAI

client = OpenAI(base_url=f"<server_url>/v1", api_key="<api-key>")

IMG_PATH = "./test.jpg"

# Function to encode a local image into data URL
def local_image_to_data_url(image_path):
    # Guess the MIME type of the image based on the file extension
    mime_type, _ = guess_type(image_path)
    if mime_type is None:
        mime_type = 'application/octet-stream'  # Default MIME type if none is found

    # Read and encode the image file
    with open(image_path, "rb") as image_file:
        base64_encoded_data = base64.b64encode(image_file.read()).decode('utf-8')

    # Construct the data URL
    return f"data:{mime_type};base64,{base64_encoded_data}"

response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "å¯¹æ¯”è¿™ä¸¤å¼ æŠ¥è¡¨çš„å·®å¼‚"},
                {"type": "image_url", "image_url": {"url": local_image_to_data_url(img_path)}},
            ]
        }
    ]
)

msg = response.choices[0].message
print(msg.content)
```

#### é”™è¯¯å¤„ç†

å¦‚æœ OCR å¤„ç†å¤±è´¥ï¼Œä¼šè¿”å›é”™è¯¯ä¿¡æ¯ï¼š

```
[Image 1 OCR Failed]: Request timed out. The server may be taking too long to respond.
```

æ¨¡å‹ä¼šç»§ç»­æ‰§è¡Œï¼Œä½†æ— æ³•è·å–è¯¥å›¾ç‰‡çš„æ–‡å­—å†…å®¹ã€‚

### 3. MCP å·¥å…·è°ƒç”¨

#### åŠŸèƒ½è¯´æ˜

é›†æˆ MCP (Model Context Protocol)ï¼Œæ”¯æŒæ¨¡å‹è‡ªä¸»è°ƒç”¨å¤–éƒ¨å·¥å…·ã€‚ç³»ç»Ÿä¼šï¼š
- è‡ªåŠ¨è¯†åˆ«æ¨¡å‹çš„å·¥å…·è°ƒç”¨éœ€æ±‚
- æ‰§è¡Œå·¥å…·å¹¶è·å–ç»“æœ
- å°†ç»“æœåé¦ˆç»™æ¨¡å‹ç»§ç»­æ¨ç†
- æ”¯æŒæœ€å¤š 10 è½®è¿­ä»£è°ƒç”¨

#### è¿æ¥ MCP æœåŠ¡å™¨

```bash
# è¿æ¥ MCP æœåŠ¡å™¨
curl -X POST http://your-domain:port/v1/mcp/connect \
  -H "Content-Type: application/json" \
  -d '{
    "url": "http://mcp-server:8080/sse"
  }'

# å“åº”
{
  "status": "ok",
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "search_documents",
        "description": "æœç´¢æ–‡æ¡£åº“",
        "parameters": {...}
      }
    }
  ]
}
```

#### æŸ¥è¯¢å¯ç”¨å·¥å…·

```bash
curl http://your-domain:port/v1/mcp/tools

# å“åº”
{
  "tools": [...]
}
```

#### å·¥å…·è°ƒç”¨æµç¨‹

```
ç”¨æˆ·è¯·æ±‚ â†’ æ¨¡å‹å†³ç­– â†’ è°ƒç”¨å·¥å…· â†’ è·å–ç»“æœ â†’ ç»§ç»­æ¨ç† â†’ é‡å¤æˆ–ç»“æŸ
```

æœ€å¤šæ”¯æŒ 10 è½®è¿­ä»£ï¼Œè¶…è¿‡åè¿”å›é”™è¯¯ï¼š
```json
{
  "detail": "Reached max tool call iterations"
}
```

#### æµå¼å“åº”ä¸­çš„å·¥å…·è°ƒç”¨

```json
// 1. æ¨¡å‹å‘èµ·å·¥å…·è°ƒç”¨
{
  "choices": [{
    "delta": {
      "tool_calls": [{
        "index": 0,
        "id": "call_abc123",
        "function": {
          "name": "search_documents",
          "arguments": "{\"query\":\"è´¢æŠ¥\"}"
        }
      }]
    }
  }]
}

// 2. å·¥å…·æ‰§è¡Œç»“æœ
{
  "choices": [{
    "delta": {
      "role": "tool",
      "content": "{\"results\": [...]}",
      "tool_call_id": "call_abc123"
    }
  }]
}

// 3. æ¨¡å‹åŸºäºç»“æœç»§ç»­å›ç­”
{
  "choices": [{
    "delta": {
      "content": "æ ¹æ®æ£€ç´¢ç»“æœ..."
    }
  }]
}
```

#### Python ç¤ºä¾‹

```python
# MCP å·¥å…·ä¼šè‡ªåŠ¨è¢«è°ƒç”¨ï¼Œæ— éœ€é¢å¤–é…ç½®
response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "user", "content": "å¸®æˆ‘æœç´¢æœ€æ–°çš„äº§å“æ–‡æ¡£"}
    ],
    stream=True
)

for chunk in response:
    delta = chunk.choices[0].delta
    if getattr(delta, "reasoning_content", None):
        print(delta.reasoning_content, end="", flush=True)
        reasoning_text += delta.reasoning_content
    if getattr(delta, "tool_calls", None):
        for tc in delta.tool_calls:
            if tc.function and tc.function.name:
                print(f"\nğŸ›  è°ƒç”¨å‡½æ•°: {tc.function.name}")
            if tc.function and tc.function.arguments:
                print(tc.function.arguments, end="", flush=True)
    elif getattr(delta, "role", None) == "tool":
        tool_call_id = getattr(delta, "tool_call_id", None)
        tool_name = getattr(delta, "name", None)
        tool_content = getattr(delta, "content", None)
        print(f"\nğŸ“¦ å·¥å…·è¿”å›ç»“æœ({tool_name}, id={tool_call_id}): {tool_content}")
    elif delta.content:
        print(delta.content, end="", flush=True)
        response_text += delta.content
```

#### éæµå¼å“åº”ä¸­çš„ Trace

```json
{
  "trace": [
    {
      "type": "assistant",
      "step": 0,
      "message": {...},
      "tool_calls": [...]
    },
    {
      "type": "tool_result",
      "step": 0,
      "tool_call_id": "call_abc123",
      "tool_name": "search_documents",
      "arguments": {"query": "è´¢æŠ¥"},
      "result": "{\"results\": [...]}"
    },
    {
      "type": "assistant",
      "step": 1,
      "message": {...}
    }
  ]
}
```

## ç»„åˆä½¿ç”¨ç¤ºä¾‹

### åœºæ™¯ï¼šæ™ºèƒ½æ–‡æ¡£åˆ†æåŠ©æ‰‹

ç»“åˆçŸ¥è¯†åº“æ£€ç´¢ã€å›¾åƒè¯†åˆ«ã€å·¥å…·è°ƒç”¨ä¸‰å¤§èƒ½åŠ›ï¼š

```python
import base64
from mimetypes import guess_type
from openai import OpenAI

client = OpenAI(base_url=f"<server_url>/v1", api_key="<api-key>")

IMG_PATH = "./test.jpg"

# Function to encode a local image into data URL
def local_image_to_data_url(image_path):
    # Guess the MIME type of the image based on the file extension
    mime_type, _ = guess_type(image_path)
    if mime_type is None:
        mime_type = 'application/octet-stream'  # Default MIME type if none is found

    # Read and encode the image file
    with open(image_path, "rb") as image_file:
        base64_encoded_data = base64.b64encode(image_file.read()).decode('utf-8')

    # Construct the data URL
    return f"data:{mime_type};base64,{base64_encoded_data}"

response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "åˆ†æè¿™å¼ è´¢æŠ¥æˆªå›¾ï¼Œå¹¶å¯¹æ¯”çŸ¥è¯†åº“ä¸­çš„å†å²æ•°æ®"},
                {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}
            ]
        }
    ],
    extra={
        "file_search": {
            "vector_store_ids": ["finance-kb"],
            "max_num_results": 5
        }
    },
    stream=True
)

# å¤„ç†æµå¼å“åº”
for chunk in response:
    delta = chunk.choices[0].delta
    if getattr(delta, "reasoning_content", None):
        print(delta.reasoning_content, end="", flush=True)
    if getattr(delta, "tool_calls", None):
        for tc in delta.tool_calls:
            if tc.function and tc.function.name:
                print(f"\nğŸ›  è°ƒç”¨å‡½æ•°: {tc.function.name}")
            if tc.function and tc.function.arguments:
                print(tc.function.arguments, end="", flush=True)
    if getattr(delta, "role", None) == "file_search":
        annotations = getattr(delta, "annotations", [])
        if annotations:
            print("\nğŸ“„ RAG æŸ¥è¯¢ç»“æœï¼š", flush=True)
            for i, ann in enumerate(annotations, start=1):
                doc_id = ann.get("id", "")
                filename = ann.get("filename", "")
                print(f"  {i}. æ–‡æ¡£ID: {doc_id}", flush=True)
                print(f"     æ–‡ä»¶å: {filename}", flush=True)
                # ä»…æ˜¾ç¤ºå†…å®¹å‰ 100 ä¸ªå­—ç¬¦ä½œä¸ºé¢„è§ˆ
                preview = ann.get("content", "")
                preview = preview.replace("\n", " ")[:100]
                print(f"     å†…å®¹é¢„è§ˆ: {preview}...", flush=True)
                print("  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€", flush=True)
    elif getattr(delta, "role", None) == "tool":
        tool_call_id = getattr(delta, "tool_call_id", None)
        tool_name = getattr(delta, "name", None)
        tool_content = getattr(delta, "content", None)
        print(f"\nğŸ“¦ å·¥å…·è¿”å›ç»“æœ({tool_name}, id={tool_call_id}): {tool_content}")
    elif delta.content:
        print(delta.content, end="", flush=True)

```

## æŠ€æœ¯è§„æ ¼

### é™åˆ¶è¯´æ˜

| é¡¹ç›® | é™åˆ¶ |
|------|------|
| MCP å·¥å…·è°ƒç”¨è¿­ä»£æ¬¡æ•° | æœ€å¤š 10 è½® |
| OCR è¯·æ±‚è¶…æ—¶ | 30 ç§’ |
| çŸ¥è¯†åº“æ£€ç´¢è¶…æ—¶ | ä¾èµ–åç«¯æœåŠ¡é…ç½® |
| å›¾ç‰‡æ•°é‡ | å»ºè®®ä¸è¶…è¿‡ 5 å¼ /è¯·æ±‚ |

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **çŸ¥è¯†åº“æ£€ç´¢**
   - åˆç†è®¾ç½® `max_num_results`ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
   - RAG æ¨¡å¼å»ºè®® 5-10 æ¡ï¼ŒKAG æ¨¡å¼å»ºè®® 10-20 æ¡

2. **å›¾åƒå¤„ç†**
   - å»ºè®®å‹ç¼©å›¾ç‰‡è‡³ 1024px ä»¥å†…
   - ä½¿ç”¨é«˜è´¨é‡æ‰«æä»¶ä»¥æå‡ OCR å‡†ç¡®ç‡

3. **æµå¼è¾“å‡º**
   - å¯¹äºé•¿æ–‡æœ¬ç”Ÿæˆåœºæ™¯å¼ºçƒˆå»ºè®®å¯ç”¨ `stream: true`
   - æå‡ç”¨æˆ·ä½“éªŒï¼Œå‡å°‘ç­‰å¾…æ—¶é—´

## é”™è¯¯å¤„ç†

### å¸¸è§é”™è¯¯

```json
// 1. API Key æ— æ•ˆ
{
  "detail": "API Key æ— æ•ˆ: Invalid or expired key"
}

// 2. æ¨¡å‹æœªé…ç½®
{
  "error": {
    "message": "Model 'xxx' not found in database.",
    "type": "invalid_request_error",
    "param": "model",
    "code": "model_not_found"
  }
}

// 3. MCP æœªè¿æ¥
{
  "detail": "MCPè¿æ¥å¤±è´¥: Connection refused"
}

// 4. RAG æ£€ç´¢å¤±è´¥
// æµå¼å“åº”ä¸­ä¼šè·³è¿‡æ£€ç´¢ï¼Œç›´æ¥åŸºäºåŸå§‹ä¸Šä¸‹æ–‡å›ç­”
// éæµå¼å“åº”ä¸­ä¼šåœ¨ trace ä¸­è®°å½•é”™è¯¯
{
  "trace": [
    {
      "type": "rag_error",
      "message": "Connection timeout"
    }
  ]
}
```
