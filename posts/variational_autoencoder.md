---
description: VAE(Variational Autoencoder)简单推导及理解
date: '2024-03-12'
author: 'pxx'
categories:
  - Deep Learning
published: true
---



# 变分自编码器（VAE）

## 信息量以及熵

[【10分钟】了解香农熵，交叉熵和KL散度](https://www.bilibili.com/video/BV1JY411q72n?vd_source=eac89beacf4b5ecfa9a66e7ebc9bd301)

信息量是一个事件发生所提供的信息的度量。在信息论中，我们通常使用信息量来表示事件的不确定性或意外性。信息量的概念源于香农的信息论，被用来衡量消息或事件的信息量大小。

信息量与事件的概率有关，通常用信息量函数 $ I(x) $ 来表示，它与事件发生的概率 $ P(x) $ 成正比，即：

$$
I(x) = - \log P(x)
$$
其中， $x$ 是一个事件，$ P(x) $ 是该事件发生的概率。信息量函数的基数通常是2，表示以二进制为基础的信息单位（比特），但也可以是其他基数，比如自然对数的基数 $ e $。

**信息量的直观解释是：事件的概率越小，发生时提供的信息量越大；而事件的概率越大，提供的信息量越小**。举个例子，如果一个非常罕见的事件发生了，它提供的信息量会很高，因为它的发生相对于预期是非常意外的；相反，一个常见的事件发生时提供的信息量会很低，因为它的发生是比较预期的。

熵是信息论中用来衡量随机变量不确定性的概念。在不同的上下文中，有几种不同类型的熵。

1. **香农熵（Shannon Entropy）**：
   香农熵是信息论中最常见的一种熵。它用来衡量一个随机变量的不确定性，定义为该随机变量所有可能取值的概率分布的加权平均值的期望。香农熵的计算公式如下：
   $$
   H(X) = - \sum_{i} p(x_i) \log p(x_i) 
   $$
   其中，$ p(x_i) $ 是随机变量 $ X $ 取值为 $ x_i $ 的概率。

2. **条件熵（Conditional Entropy）**：
   条件熵是在给定另一个随机变量的情况下，一个随机变量的不确定性。它衡量了一个随机变量对另一个随机变量提供的平均信息量，即在给定某些条件下，随机变量的不确定性。条件熵的计算公式如下：
   $$
   H(X|Y) = - \sum_{y} \sum_{x} p(x,y) \log \frac{p(x,y)}{p(y)}
   $$
   其中，$ p(x,y) $ 是随机变量 $ X $ 和 $ Y $ 同时取值为 $ x $ 和 $ y $ 的联合概率，$ p(y) $ 是随机变量 $ Y $ 的概率。

3. **相对熵（Kullback-Leibler Divergence）**：
   相对熵衡量了两个概率分布之间的差异或不相似程度。它的计算公式如下：
   $$
   D_{KL}(P||Q) = \sum_{x} p(x) \log \frac{p(x)}{q(x)}
   $$
   其中，$ P $ 和 $ Q $ 是两个概率分布，$ p(x) $ 和 $ q(x) $ 分别是这两个分布对应事件 $ x $ 的概率。

4. **交叉熵（Cross Entropy）**

   交叉熵是信息论中另一种重要的概念，它常用于衡量两个概率分布之间的差异。具体来说，它衡量了使用一个概率分布来编码事件的平均比特数，但是这些事件的真实分布是另一个概率分布。在机器学习中，交叉熵经常用作损失函数，特别是在分类问题中。

   假设我们有两个概率分布 $ P(x) $ 和 $ Q(x) $，其中 $ x $ 是事件的一个可能取值。交叉熵 $ H(P, Q) $ 用来衡量当使用概率分布 $ Q(x) $ 来编码事件时，事件的平均编码长度。其计算公式如下：

   $$
   H(P, Q) = - \sum_{x} P(x) \log Q(x)
   $$
   其中，$ P(x) $ 是事件的真实分布，$ Q(x) $ 是用来编码事件的近似分布。

   在机器学习中，通常我们有一组观测数据 $ \{ x_i \} $，以及对应的真实标签 $ \{ y_i \} $。如果我们使用一个神经网络模型生成的概率分布 $ P(y|x) $ 来预测标签，那么交叉熵损失可以表示为：

   $$
   H(y, \hat{y}) = - \sum_{i} y_i \log \hat{y}_i
   $$
   其中，$ y_i $ 是第 $ i $ 个样本的真实标签的概率分布，$ \hat{y}_i $ 是模型预测的标签的概率分布。优化这个交叉熵损失函数能够使得模型的预测尽可能接近真实标签的分布。

这些熵的概念在信息论、统计学和机器学习等领域中都有广泛的应用。它们可以用来描述信息的不确定性、计算两个概率分布之间的差异以及进行模型选择等任务。

## 变分推理

[【15分钟】了解变分推理](https://www.bilibili.com/video/BV1Gs4y157BU?vd_source=eac89beacf4b5ecfa9a66e7ebc9bd301)

贝叶斯公式描述了在观察到数据 $D$ 后，我们如何更新参数 $\theta$ 的后验概率分布 $P(\theta|D)$。其公式如下所示：

$$
P(\theta|D) = \frac{P(D|\theta) \times P(\theta)}{P(D)}
$$
其中：

- $ P(\theta|D) $ 是给定观测数据 $ D $后参数 $ \theta $ 的后验概率分布。
- $ P(D|\theta) $ 是在给定参数 $ \theta $ 下观测到数据 $ D $ 的似然函数。
- $ P(\theta) $ 是参数 $ \theta $ 的先验概率分布。
- $ P(D) $ 是观测数据 $ D $ 的边缘概率，也称为边缘似然。

边缘概率计算公式如下所示：

$$
P(D) = \int P(D|\theta) \times P(\theta) \, d\theta
$$
这里的 $ P(D) $ 表示观测数据 $D$ 的边缘概率，是在所有可能的参数值下观测到数据的概率的总和。

**生成模型描述了如何从潜在变量生成观测数据，而变分推断旨在近似给定观测数据的潜在变量的后验分布。**

变分推理（Variational Inference）是一种用于近似求解复杂概率模型后验分布的推理方法。在贝叶斯推断中，我们通常希望计算后验分布，即给定观测数据的情况下，模型参数的分布。然而，对于许多实际问题，后验分布往往是难以解析求解的，尤其是在高维或复杂模型的情况下。

变分推理通过引入一个简单的参数化分布来近似真实的后验分布。这个参数化分布通常被称为变分分布，记作 $q(\theta)$。然后，变分推理的目标是通过最大化一个称为**变分下界（ELBO）**的函数来优化变分分布的参数，使得它尽可能接近真实的后验分布。ELBO定义了观测数据的对数似然和变分分布的**KL散度**之间的关系，具体公式如下：

$$
\text{ELBO}(\theta) = \mathbb{E}_{q(\theta)}[\log P(X, \theta)] - \mathbb{E}_{q(\theta)}[\log q(\theta)]
$$
其中 $P(X, \theta)$ 是观测数据 $X$ 和模型参数$\theta$ 的联合概率分布。通过最大化 **ELBO**，我们调整变分分布的参数，使得它尽可能接近真实的后验分布。

变分推理是一种灵活且高效的推理方法，适用于各种复杂的概率模型。它在处理大规模数据和高维模型时具有优势，并且可以应用于许多领域，如机器学习、统计学和人工智能。

## Variational Autoencoder

[变分自编码器 VAE 鲁鹏](https://www.bilibili.com/video/BV1Zq4y1h7Tu?vd_source=eac89beacf4b5ecfa9a66e7ebc9bd301)
